<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>PIO: Install Walk-through</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">PIO
   &#160;<span id="projectnumber">2.4.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Install Walk-through </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This document provides specific instructions for installing PIO using a variety of compilers on a few commonly used super computers. Click on the link below to go directly to the machine of interest.</p>
<ul>
<li><a href="#Yellowstone">Yellowstone</a> (NCAR's 1.5-petaflop IBM Supercomputer)</li>
<li><a href="#Edison">Edison</a> (A NERSC Cray XC30 Supercomputer)</li>
<li><a href="#Mira">Mira</a> (IBM Blue Gene Supercomputer at ALCF)</li>
<li><a href="#BlueWat">Blue Waters</a> (NCSA's 1.3-petaflop Cray Supercomputer)</li>
<li><a href="#Hobart">Hobart</a> (The NCAR CGD local cluster)</li>
<li><a href="#Linux">Linux with MPICH</a> (Standard Linux box with MPICH)</li>
</ul>
<p><a class="anchor" id="Yellowstone"></a> </p><h3>Yellowstone</h3>
<ol>
<li>
<p class="startli">Directory setup</p>
<p>Download a copy of the PIO source into a sub-directory of your working directory (refered to here as the PIO_source directory). Create another sub-directory for the build (refered to here as the PIO_build directory) and 'cd' into it.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Modules</p>
<p>Modules required for installation depend on your prefered compiler. Issue the commands below to set the module environment for building PIO on Yellowstone.</p>
<ul>
<li><p class="startli">Intel</p>
<p class="startli">%&gt; module reset<br />
 %&gt; module unload netcdf<br />
 %&gt; module swap intel intel/15.0.3<br />
 %&gt; module load git/2.3.0<br />
 %&gt; module load cmake/3.0.2<br />
 %&gt; module load netcdf-mpi/4.3.3.1<br />
 %&gt; module load pnetcdf/1.6.1<br />
</p>
</li>
<li><p class="startli">GNU</p>
<p class="startli">%&gt; module reset<br />
 %&gt; module unload netcdf<br />
 %&gt; module swap intel gnu/4.8.2<br />
 %&gt; module load git/2.3.0<br />
 %&gt; module load cmake/3.0.2<br />
 %&gt; module load netcdf-mpi/4.3.3.1<br />
 %&gt; module load pnetcdf/1.6.1<br />
</p>
</li>
<li><p class="startli">PGI</p>
<p class="startli">%&gt; module reset<br />
 %&gt; module unload netcdf<br />
 %&gt; module swap intel pgi/13.3<br />
 %&gt; module load git/2.3.0<br />
 %&gt; module load cmake/3.0.2<br />
 %&gt; module load netcdf-mpi/4.3.3.1<br />
 %&gt; module load pnetcdf/1.6.1<br />
</p>
</li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli">Environment Variables</p>
<p>The appropriate compiler wrappers must be chosen for cmake, so the environment variables CC and FC must be set as:<br />
 CC=mpicc<br />
 FC=mpif90<br />
</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Build</p>
<p class="endli">Building PIO requires running the CMake configure and then make. In the PIO_build directory type<br />
 %&gt; cmake ../PIO_source/<br />
 %&gt; make<br />
 </p>
</li>
</ol>
<p><a class="anchor" id="Edison"></a> </p><h3>Edison</h3>
<ol>
<li>
<p class="startli">Directory setup</p>
<p>Download a copy of the PIO source into a sub-directory of your working directory (refered to here as the PIO_source directory). Create another sub-directory for the build (refered to here as the PIO_build directory) and 'cd' into it.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Modules</p>
<p>Modules required for installation depend on your prefered compiler. Issue the commands below to set the module environment for building PIO on Edison.</p>
<ul>
<li><p class="startli">Intel</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load PrgEnv-intel<br />
 %&gt; module load craype-ivybridge<br />
 %&gt; module load cray-shmem<br />
 %&gt; module load cray-mpich<br />
 %&gt; module load torque<br />
 %&gt; module load git/2.4.6 <br />
 %&gt; module load cmake/3.0.0<br />
 %&gt; module load cray-hdf5-parallel/1.8.14<br />
 %&gt; module load cray-netcdf-hdf5parallel/4.3.3.1<br />
 %&gt; module load cray-parallel-netcdf/1.6.1<br />
</p>
</li>
<li><p class="startli">GNU</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load PrgEnv-gnu<br />
 %&gt; module load craype-ivybridge<br />
 %&gt; module load cray-shmem<br />
 %&gt; module load cray-mpich<br />
 %&gt; module load torque<br />
 %&gt; module load git/2.4.6<br />
 %&gt; module load cmake/3.0.0<br />
 %&gt; module load cray-hdf5-parallel/1.8.14<br />
 %&gt; module load cray-netcdf-hdf5parallel/4.3.3.1<br />
 %&gt; module load cray-parallel-netcdf/1.6.1<br />
</p>
</li>
<li><p class="startli">Cray</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load PrgEnv-cray<br />
 %&gt; module load craype-ivybridge<br />
 %&gt; module load cray-shmem<br />
 %&gt; module load cray-mpich<br />
 %&gt; module swap cce cce/8.4.0.223<br />
 %&gt; module load torque<br />
 %&gt; module load git/2.4.6 <br />
 %&gt; module load cmake/3.0.0<br />
 %&gt; module load cray-hdf5-parallel/1.8.14<br />
 %&gt; module load cray-netcdf-hdf5parallel/4.3.3.1<br />
 %&gt; module load cray-parallel-netcdf/1.6.1<br />
</p>
</li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli">Environment Variables</p>
<p>The appropriate compiler wrappers must be chosen for cmake, so the environment variables CC and FC must be set as:<br />
 CC=cc<br />
 FC=ftn<br />
</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Build</p>
<p>Building PIO requires running the CMake configure and then make. In the PIO_build directory type<br />
 %&gt; cmake -DCMAKE_VERBOSE_MAKEFILE=TRUE \<br />
 -DPREFER_STATIC=TRUE \<br />
 -DNetCDF_PATH=${NETCDF_DIR} \<br />
 -DPnetCDF_PATH=${PARALLEL_NETCDF_DIR} \<br />
 -DHDF5_PATH=${HDF5_DIR} \<br />
 -DMPI_C_INCLUDE_PATH=${MPICH_DIR}/include \<br />
 -DMPI_Fortran_INCLUDE_PATH=${MPICH_DIR}/include \<br />
 -DMPI_C_LIBRARIES=${MPICH_DIR}/lib/libmpich.a \<br />
 -DMPI_Fortran_LIBRARIES=${MPICH_DIR}/lib/libmpichf90.a \<br />
 -DCMAKE_SYSTEM_NAME=Catamount \<br />
 ../PIO_source/<br />
 %&gt; make</p>
<p class="endli"></p>
</li>
</ol>
<p><a class="anchor" id="Mira"></a> </p><h3>Mira/Cetus</h3>
<ol>
<li>
<p class="startli">Directory setup</p>
<p>Download a copy of the PIO source into a sub-directory of your working directory (refered to here as the PIO_source directory). Create another sub-directory for the build (refered to here as the PIO_build directory) and 'cd' into it.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Softenv packages and environment variables</p>
<p>It is not necessary to edit your .soft file on Mira inorder to build PIO. Execute the following commands to temporarily load packages into your softenv. These packages use the IBM/XL compiler.<br />
 %&gt; soft add +mpiwrapper-xl (or switch from the default in your softenv)<br />
 %&gt; soft add @ibm-compilers-2015-02<br />
 %&gt; soft add +cmake<br />
 %&gt; soft add +git<br />
</p>
<p>And then set the following environment variables to add in the rest of the library paths.<br />
 %&gt; setenv LIBZ /soft/libraries/alcf/current/xl/ZLIB<br />
 %&gt; setenv HDF5 /soft/libraries/hdf5/1.8.14/cnk-xl/V1R2M2-20150213<br />
 %&gt; setenv NETCDF /soft/libraries/netcdf/4.3.3-f4.4.1/cnk-xl/V1R2M2-20150213<br />
 %&gt; setenv PNETCDF /soft/libraries/pnetcdf/1.6.1/cnk-xl/V1R2M2-20150213<br />
 %&gt; setenv CC /soft/compilers/wrappers/xl/mpixlc_r<br />
 %&gt; setenv FC /soft/compilers/wrappers/xl/mpixlf90_r<br />
</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Build</p>
<p>Building PIO requires running the CMake configure and then make. In the PIO_build directory type<br />
 %&gt; cmake -DPREFER_STATIC=TRUE ../PIO_source/<br />
 %&gt; make</p>
<p class="endli"></p>
</li>
</ol>
<p><a class="anchor" id="BlueWat"></a> </p><h3>Blue Waters</h3>
<ol>
<li>
<p class="startli">Directory setup</p>
<p>Download a copy of the PIO source into a sub-directory of your working directory (refered to here as the PIO_source directory). Create another sub-directory for the build (refered to here as the PIO_build directory) and 'cd' into it.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Modules</p>
<p>Modules required for installation depend on your prefered compiler. Issue the commands below to set the module environment for building PIO on Hobart.</p>
<ul>
<li><p class="startli">Intel</p>
<p class="startli">%&gt; module swap PrgEnv-cray PrgEnv-intel<br />
 %&gt; module load torque<br />
 %&gt; module load git<br />
 %&gt; module load cmake<br />
 %&gt; module load cray-hdf5-parallel/1.8.14<br />
 %&gt; module load cray-netcdf-hdf5parallel/4.3.3.1<br />
 %&gt; module load cray-parallel-netcdf/1.6.1<br />
</p>
</li>
<li><p class="startli">PGI</p>
<p class="startli">%&gt; module swap PrgEnv-cray PrgEnv-pgi<br />
 %&gt; module load torque<br />
 %&gt; module load git<br />
 %&gt; module load cmake<br />
 %&gt; module load cray-hdf5-parallel/1.8.14<br />
 %&gt; module load cray-netcdf-hdf5parallel/4.3.3.1<br />
 %&gt; module load cray-parallel-netcdf/1.6.1<br />
</p>
</li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli">Environment Variables</p>
<p>The appropriate compiler wrappers must be chosen for cmake, so the environment variables CC and FC must be set as:<br />
 CC=cc<br />
 FC=ftn<br />
</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Build</p>
<p>Building PIO requires running the CMake configure and then make. In the PIO_build directory type<br />
 %&gt; cmake -DCMAKE_VERBOSE_MAKEFILE=TRUE \<br />
 -DPREFER_STATIC=TRUE \<br />
 -DNetCDF_PATH=${NETCDF_DIR} \<br />
 -DPnetCDF_PATH=${PARALLEL_NETCDF_DIR} \<br />
 -DHDF5_PATH=${HDF5_DIR} \<br />
 -DMPI_C_INCLUDE_PATH=${MPICH_DIR}/include \<br />
 -DMPI_Fortran_INCLUDE_PATH=${MPICH_DIR}/include \<br />
 -DMPI_C_LIBRARIES=${MPICH_DIR}/lib/libmpich.a \<br />
 -DMPI_Fortran_LIBRARIES=${MPICH_DIR}/lib/libmpichf90.a \<br />
 -DCMAKE_SYSTEM_NAME=Catamount \<br />
 ../PIO_source/<br />
 %&gt; make</p>
<p class="endli"></p>
</li>
</ol>
<p><a class="anchor" id="Hobart"></a> </p><h3>Hobart</h3>
<ol>
<li>
<p class="startli">Directory setup</p>
<p>Download a copy of the PIO source into a sub-directory of your working directory (refered to here as the PIO_source directory). Create another sub-directory for the build (refered to here as the PIO_build directory) and 'cd' into it.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Modules</p>
<p>Modules required for installation depend on your prefered compiler. Issue the commands below to set the module environment for building PIO on Hobart.</p>
<ul>
<li><p class="startli">Intel</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load compiler/intel/15.0.2.164<br />
 %&gt; module load tool/parallel-netcdf/1.6.1/intel<br />
</p>
</li>
<li><p class="startli">Nag</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load compiler/nag/6.0<br />
 %&gt; module load tool/parallel-netcdf/1.6.1/nag/openmpi<br />
</p>
</li>
<li><p class="startli">PGI</p>
<p class="startli">%&gt; module purge<br />
 %&gt; module load compiler/pgi/15.1<br />
 %&gt; module load tool/parallel-netcdf/1.6.1/pgi/mvapich2<br />
</p>
</li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli">Environment Variables</p>
<p>The appropriate compiler wrappers must be chosen for cmake, so the environment variables CC and FC must be set as:<br />
 CC=mpicc<br />
 FC=mpif90<br />
</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Build</p>
<p>Building PIO requires running the CMake configure and then make. In the PIO_build directory type<br />
 %&gt; cmake -DNETCDF_DIR=$NETCDF_PATH -DPNETCDF_DIR=$PNETCDF_PATH ../PIO_source/<br />
 %&gt; make</p>
<p>ParallelIO does not require Parallel netcdf to run, so if you decide to use the GNU compiler on Hobart (not described here) without the parallel-netcdf library, use the cmake configure flags:<br />
 %&gt; cmake -DNETCDF_DIR=$NETCDF_PATH -DWITH_PNETCDF=FALSE ../PIO_source/<br />
 %&gt; make</p>
<p class="endli"></p>
</li>
</ol>
<p><a class="anchor" id="Linux"></a> </p><h3>Linux with MPICH</h3>
<ol>
<li>
<p class="startli">Installing MPICH</p>
<p></p>
<p>Download from the <a href="http://www.mpich.org/downloads/">MPICH2 downloads page</a>. (These instructions were tested using version 3.2). Untar with: </p><pre>tar zxf mpich-3.2.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd mpich-3.2 &amp;&amp; ./configure --prefix=/usr/local &amp;&amp; make all check
&amp;&amp; sudo make install </pre><p></p>
<p>Now you should be able to access mpicc, mpifort, and mpirun from the command line. (If not, make sure /usr/local/bin is in path.)</p>
<p></p>
<p>Note that if you wish to use valgrind on your programs, you should configure MPICH like this: </p><pre>CPPFLAGS=-I/usr/include/valgrind ./configure --prefix=/usr/local --enable-g=mem,meminit</pre><p></p>
<p>For this to work you must have packages valgrind and valgrind-devel installed.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Installing Zlib</p>
<p></p>
<p>Download from the <a href="http://www.zlib.net/">zlib downloads page</a>. (These instructions were tested using version 1.2.11). Untar with: </p><pre>tar zxf zlib-1.2.11.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd zlib-1.2.11
CC=mpicc ./configure --prefix=/usr/local/zlib-1.2.11_mpich-3.2
make all check
sudo make install </pre><p class="endli"></p>
</li>
<li>
<p class="startli">Installing SZlib</p>
<p></p>
<p>Download szlib from the <a href="https://www.hdfgroup.org/doc_resource/SZIP/">HDF5 szlib page</a>. (These instructions were tested using version 2.1). Untar with: </p><pre>tar zxf szip-2.1.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd slib-2.1
CC=mpicc ./configure --prefix=/usr/local/szip-2.1_mpich-3.2
make all check
sudo make install </pre><p class="endli"></p>
</li>
<li>
<p class="startli">Installing pNetCDF</p>
<p></p>
<p>Download parallel-netcdf from the <a href="http://cucis.ece.northwestern.edu/projects/PnetCDF/download.html">parallel-netcdf download page</a>. (These instructions were tested using version 1.8.1). Untar with: </p><pre>tar zxf parallel-netcdf-1.8.1.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd parallel-netcdf-1.8.1
FC=mpifort CC=mpicc CFLAGS=-fPIC ./configure --prefix=/usr/local/pnetcdf-1.8.1_mpich-3.2
make all check
sudo make install </pre><p class="endli"></p>
</li>
<li>
<p class="startli">Installing HDF5</p>
<p></p>
<p>Download HDF5 from the <a href="https://www.hdfgroup.org/downloads/hdf5/source-code/">HDF5 download page</a>.</p>
<p></p>
<p>These instructions were tested using version 1.10.1. Untar with: </p><pre>tar zxf hdf5-1.10.1.tar.gz</pre><p>. Note that in my case I need to add /usr/local/bin to the PATH for su, because that is where mpicc is found.</p>
<p></p>
<p>Build with: </p><pre>cd hdf5-1.10.1
CC=mpicc ./configure --with-zlib=/usr/local/zlib-1.2.11_mpich-3.2 --with-szlib=/usr/local/szip-2.1_mpich-3.2 --prefix=/usr/local/hdf5-1.10.1_mpich-3.2 --enable-parallel
make all check
sudo PATH=$PATH:/usr/local/bin make install</pre><p class="endli"></p>
</li>
<li>
<p class="startli">Installing NetCDF-4 C Library</p>
<p></p>
<p>Download the netcdf C library from the <a href="http://www.unidata.ucar.edu/downloads/netcdf/index.jsp">NetCDF download page</a>. Untar with: </p><pre>tar zxf netcdf-c-4.5.0-rc1.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd netcdf-c-4.5.0-rc1
CPPFLAGS='-I/usr/local/zlib-1.2.11_mpich-3.2 -I/usr/local/szip-2.1_mpich-3.2/include -I/usr/local/hdf5-1.10.1_mpich-3.2/include' LDFLAGS='-L/usr/local/zlib-1.2.11_mpich-3.2/lib -L/usr/local/szip-2.1_mpich-3.2/lib -L/usr/local/hdf5-1.10.1_mpich-3.2/lib' CC=mpicc ./configure --enable-parallel-tests --prefix=/usr/local/netcdf-4.4.1_mpich-3.2
make all check
sudo make install </pre><p></p>
<p>Note that you may not build netCDF with it's built-in parallel-netCDF support, if you are also planning to use the parallel-netCDF library with PIO. For PIO, parallel-netCDF must be installed independently of netCDF.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Installing NetCDF-4 Fortran Library</p>
<p></p>
<p>Download the netcdf Fortran library from the <a href="http://www.unidata.ucar.edu/downloads/netcdf/index.jsp">NetCDF download page</a>. Untar with: </p><pre>tar zxf netcdf-fortran-4.4.4.tar.gz</pre><p></p>
<p>Build with: </p><pre>cd netcdf-fortran-4.4.4
CC=mpicc LD_LIBRARY_PATH=/usr/local/netcdf-4.4.1_mpich-3.2/lib FC=mpifort CPPFLAGS=-I/usr/local/netcdf-4.4.1_mpich-3.2/include LDFLAGS=-L/usr/local/netcdf-4.4.1_mpich-3.2/lib ./configure --enable-parallel-tests --prefix=/usr/local/netcdf-fortran-4.4.4_c_4.4.1_mpich-3.2
make all
sudo make install </pre><p></p>
<p>Note that make check did not work because of a failure in a test. This has been reported to the netCDF team.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Installing ParallelIO Library</p>
<p></p>
<p>Clone the ParallelIO library.</p>
<p></p>
<p>Create a build directory and run cmake. </p><pre>
cd ParallelIO
mkdir build
cd build
CC=mpicc FC=mpifort cmake -DNetCDF_C_PATH=/usr/local/netcdf-4.4.1_mpich-3.2 -DNetCDF_Fortran_PATH=/usr/local/netcdf-fortran-4.4.4_c_4.4.1_mpich-3.2 -DPnetCDF_PATH=/usr/local/pnetcdf-1.8.1_mpich-3.2 -DPIO_HDF5_LOGGING=On -DPIO_USE_MALLOC=On ..
make
make check
sudo make install
</pre><p></p>
<p>When debugging build issues, it's helpful to be able to do a clean build from the command line, including tests:</p>
<pre>
cd build
rm -rf * &amp;&amp; CFLAGS='-Wall -g' FFLAGS=-g CC=mpicc FC=mpifort cmake -DNetCDF_C_PATH=/usr/local/netcdf-4.4.1_mpich-3.2 -DNetCDF_Fortran_PATH=/usr/local/netcdf-fortran-4.4.4_c_4.4.1_mpich-3.2 -DPnetCDF_PATH=/usr/local/pnetcdf-1.8.1_mpich-3.2 -DPIO_HDF5_LOGGING=On -DPIO_USE_MALLOC=On -DPIO_ENABLE_LOGGING=On .. &amp;&amp; make VERBOSE=1 all tests check
</pre><p></p>
<p>To build with address sanitizer:</p>
<pre>
rm -rf * &amp;&amp; CFLAGS='-Wall -g  -fsanitize=address -fno-omit-frame-pointer' FFLAGS='-g  -fsanitize=address -fno-omit-frame-pointer' CC=mpicc FC=mpifort cmake -DNetCDF_C_PATH=/usr/local/netcdf-4.4.1_mpich-3.2 -DNetCDF_Fortran_PATH=/usr/local/netcdf-fortran-4.4.4_c_4.4.1_mpich-3.2 -DPnetCDF_PATH=/usr/local/pnetcdf-1.8.1_mpich-3.2 -DPIO_HDF5_LOGGING=On -DPIO_USE_MALLOC=On -DPIO_ENABLE_LOGGING=On .. &amp;&amp; make VERBOSE=1 all tests check
</pre><p></p>
<p>Note the optional CFLAGS=-g which allows the use of a debugger with this code. Also note the optional VERBOSE=1 provided to make, which causes the build commands to be made visible.</p>
<p></p>
<p>Note also the -DPIO_ENABLE_LOGGING=On which is helpful for debugging but should probably not be used in production builds.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli">Building PIO with autotools.</p>
<p></p>
<p>To build the PIO library with autotools, clone the repo and use a command like this:</p>
<pre>
autoreconf -i &amp;&amp; LD_LIBRARY_PATH=/usr/local/netcdf-4.4.1_mpich-3.2/lib CC=mpicc CFLAGS='-g' CPPFLAGS='-I/usr/local/netcdf-4.4.1_mpich-3.2/include/ -I/usr/local/pnetcdf-1.8.1_mpich-3.2/include' LDFLAGS='-L/usr/local/netcdf-4.4.1_mpich-3.2/lib -L/usr/local/pnetcdf-1.8.1_mpich-3.2/lib' ./configure &amp;&amp; make check
</pre><p></p>
<p>To build with debug logging and the address sanitizer for memory checking (debugging builds only!):</p>
<pre>
autoreconf -i &amp;&amp; LD_LIBRARY_PATH=/usr/local/netcdf-4.4.1_mpich-3.2/lib CC=mpicc CFLAGS='-g -fsanitize=address -fno-omit-frame-pointer' CPPFLAGS='-I/usr/local/netcdf-4.4.1_mpich-3.2/include/ -I/usr/local/pnetcdf-1.8.1_mpich-3.2/include' LDFLAGS='-L/usr/local/netcdf-4.4.1_mpich-3.2/lib -L/usr/local/pnetcdf-1.8.1_mpich-3.2/lib' ./configure --enable-logging &amp;&amp; make check
 </pre><p class="endli"></p>
</li>
<li>
<p class="startli">Building and Running Performance Tests</p>
<p></p>
<p>Download a decomp file from our <a href="http://parallelio.googlecode.com/svn/perfdecomps/trunk/30/">google code page</a>. You can use any of those files, save them to build/test/performance. (These instructions were tested with the first one in the list.)</p>
<p></p>
<p>Create a namelist file, named "pioperf.nl". Save this file in the subdirectory (note that it is in the BUILD directory): </p><pre>
build/tests/performance/
</pre><p></p>
<p>The contents of the namelist file should look like: </p><pre>
&amp;pioperf
decompfile = "/u/sciteam/thayerca/scratch/pio_work/piodecomp30tasks01dims06.dat"
pio_typenames = 'pnetcdf'
niotasks = 30
rearrangers = 1
nvars = 2
/
</pre><p></p>
<p>You should change the path to your decomp file to wherever you saved it. You can add items to the list to run more tests, so, for instance, to test all of the types of io, your pio_typenames would look like: pio_typenames = 'pnetcdf','netcdf','netcdf4p','netcdf4c'</p>
<p></p>
<p>HDF5 is netcdf4p, and Parallel-Netcdf is pnetcdf.</p>
<p></p>
<p>Example to test with different numbers of tasks, you could do: niotasks = 30,15,5</p>
<p></p>
<p>Example to test with both of the rearranger algorithms: rearrangers = 1,2</p>
<p></p>
<p>Example to test with different numbers of variables: nvars = 8,5,3,2</p>
<p></p>
<p>Once you have your decomp file and your namelist set up, run like this:</p>
<pre>
mpiexec -n 30 ./pioperf
</pre><p></p>
<p>If you run the most basic performance tests (as in the first pioperf.nl example), this script should only take a few minutes (less than 5) to complete. The output and results will be written to your pioperf.o$PBS_JOBID file. Adding more iotypes and rearrangers and variables to these tests will increase the time significantly.</p>
<p class="endli"></p>
</li>
</ol>
<p><em>Last updated: 05-16-2016</em> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed May 22 2019 15:09:23 for PIO by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
